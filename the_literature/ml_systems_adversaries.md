# ML Systems Adversaries
[A Multiscale Discriminative Attack Method for Automatic Modulation Classification](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10793417)

[The Threat of Adversarial Attacks Against Machine Learning in Network Security: A Survey](https://arxiv.org/pdf/1911.02621)
- Extensive categorization of adversarial attacks
- However, these are in the networks categories, tangentially related to physical layer wireless

[Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by Harnessing AI](https://arxiv.org/pdf/2411.14299)
- Similar to previous, though this is a dataset, and it is analog circuits
- So tangential relation but in the opposite direction where this is physical layer, but not wireless

[Defending AI-Based Automatic Modulation Recognition Models Against Adversarial Attacks](https://digitalcommons.odu.edu/cgi/viewcontent.cgi?article=1203&context=engtech_fac_pubs)

[Intriguing properties of neural networks](https://arxiv.org/pdf/1312.6199)
- This is to my knowledge the first exploration of fooling image classification neural networks

[Explaining and Harnessing Adversarial Examples](https://arxiv.org/pdf/1412.6572)
- This was a follow-up to the previous paper, this time citing actual methods of accomplishing it

[Adversarial Patch](https://arxiv.org/pdf/1712.09665)
- A group which expanded upon the image classification fooling
- Made a physical object which they could place in images to fool an image classification neural network

